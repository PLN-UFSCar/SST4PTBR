{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f516a953-3599-45de-9ed4-3286ef8b4383",
   "metadata": {},
   "source": [
    "# Detecção de sarcasmo\n",
    "\n",
    "O objetivo deste notebook é desenvolver um módulo de detecção automática de sarcasmo em textos em português, mais especificamente notícias. Para isso, foi utilizado uma base de dados composta por notícias de três grandes sites brasileiros, composta por notícias sarcásticas e não sarcásticas. Para a criação do módulo, foram criados dois modelos classificadores que utilizam metodologias diferentes: Uso de algoritmos clássicos de Machine Learning + representação vetorial estática, e Fine-tuning de um modelo transformers multi-língua\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0543dc3d-2d0f-4683-a310-eb4026fd5661",
   "metadata": {},
   "source": [
    "## Descrição da estrutura e características do data set\n",
    "\n",
    "A base de dados foi retirada do repositório [PLNCrawler](https://github.com/schuberty/PLNCrawler), e é estruturada originalmente em três arquivos JSON, que correspondem a cada site de notícias de onde as notícias foram extraídas:\n",
    "- Sensacionalista: 5006 notícias sarcásticas\n",
    "- Estadão: 11272 notícias não sarcásticas\n",
    "- Revista Piauí (seção Herald): 2216 notícias sarcásticas\n",
    "\n",
    "Cada arquivo possui os seguintes campos para cada notícia:\n",
    "- is_sarcastic (ou is_sarcasm): booleano, representa o rótulo/label da notícia (sarcástica ou não)\n",
    "- article_link: string, contem a URL de onde a notícia foi extraída\n",
    "- headline: string, contem o título da notícia\n",
    "- text: string, contem o texto da notícia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73077a2e",
   "metadata": {},
   "source": [
    "Carrega as bases de cada site em formato DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d55fb6-2d0c-4579-b677-7b413a736dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from src.scripts import get_df_sensacionalista\n",
    "from src.scripts import get_df_estadao\n",
    "from src.scripts import get_df_the_piaui_herald\n",
    "\n",
    "# Carrega o arquivo em um DataFrame\n",
    "df_sensacionalista = get_df_sensacionalista()\n",
    "df_estadao = get_df_estadao()\n",
    "df_piaui = get_df_the_piaui_herald()\n",
    "df_piaui = df_piaui.rename(columns={'is_sarcasm': 'is_sarcastic'}) # Renomeia a coluna para igualizar com os outros DataFrames\n",
    "\n",
    "display(df_sensacionalista)\n",
    "display(df_estadao)\n",
    "display(df_piaui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0763876-e34b-4bf6-bdeb-65d1eaa8178e",
   "metadata": {},
   "source": [
    "Faz a união das três bases em um só DataFrame de forma equilibrada, mantendo 50% de notícias sarcásticas e 50% de não sarcásticas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960f4cd-3e68-4cfe-a454-53523fe11614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir os 3 datasets\n",
    "from src.scripts import merge_dfs\n",
    "\n",
    "df = merge_dfs(df_sensacionalista, df_estadao, df_piaui)\n",
    "\n",
    "num_sarcastic = df['is_sarcastic'].sum()\n",
    "\n",
    "print(f'Número de amostras sarcásticas: {num_sarcastic}')\n",
    "print(f'Número de amostras não sarcásticas: {len(df) - num_sarcastic}')\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5090085-21ee-4fde-a10a-c1176905a9b1",
   "metadata": {},
   "source": [
    "# Pré processamento\n",
    "\n",
    "É importante pontuar que alguns recursos de linguagem que são removidos ou normalizados durante as etapas tradicionais de pré-processamento têm influencia na classificação de ironia em textos.\n",
    "Por exemplo, sinais de pontuação podem indicar ironia. Por isso, é um parâmetro do pré-processamento remover ou não esse recurso.\n",
    "\n",
    "Sabendo disso, podem ser passados parâmetros opcionais para a função de pré-processamento que aplicam ou não a transformação.\n",
    "\n",
    "## Stemming e lemmatization\n",
    "\n",
    "> \"Stemming or lemmatization reduces words to their root form (e.g., \"running\" becomes \"run\"), making it easier to analyze language by grouping different forms of the same word.\" Fonte: https://www.ibm.com/think/topics/natural-language-processing\n",
    "\n",
    "O processo de stemming e lemmatization são opcionais, mas ambos nunca podem ser aplicados juntos porque eles têm o mesmo propósito com abordagens diferentes.\n",
    "Dessa forma, se ambos forem ativados só o **lemmatization** será aplicado (por ser mais semântico).\n",
    "\n",
    "### Fontes para o pré-processamento:\n",
    "\n",
    "1. [Orientações principais](https://github.com/sharadpatell/Text_preprocessing_steps_for_NLP/blob/main/Text_preprocessing_steps_for_NLP.ipynb) que auxiliaram no passo a passo do pré-processamento.\n",
    "2. FACELI, K. et al. Inteligência Artificial Uma Abordagem de Aprendizado de Máquina. 2o edição ed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5387a47-0606-48fa-ae53-64d9faf76ec0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.preprocessamento import pre_processamento\n",
    "\n",
    "usar_lemmatization = False\n",
    "usar_stemming      = False\n",
    "\n",
    "df = pre_processamento(df, usar_stemming = usar_stemming, usar_lemmatization = usar_lemmatization)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7899157-7fc8-4692-891d-451fee3b3ebf",
   "metadata": {},
   "source": [
    "## Primeira abordagem para deteccção: Uso de algoritmos clássicos de Machine Learning + representação vetorial estática\n",
    "\n",
    "Para essa abordagem, o primeiro passo é criar a representação vetorial do texto, pois os computadores não interpretam os textos na linguagem do ser humano. Por isso, é necessário transformá-los para uma representação estruturada que as máquinas consigam processar.\n",
    "\n",
    "Esse tratamento do texto também faz parte da etapa de feature extraction.\n",
    "> Feature extraction is the process of converting raw text into numerical representations that machines can analyze and interpret. Fonte: https://www.ibm.com/think/topics/natural-language-processing\n",
    "\n",
    "Foi escolhido usar a ferramenta Word2Vec para a geração de vetores densos, que capturam o valor semântico das palavras e as relacionam entre sí. É ideal para tarefas de aprendizado de máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5232b5-8124-4260-939c-c2d77e859fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "usar_word2vec = False\n",
    "usar_sequence_transformer = not usar_word2vec\n",
    "usar_sequence_transformer = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da54478",
   "metadata": {},
   "source": [
    "Aplica Word2Vec na base de dados, gerando a rede neural e retornando os embeddings para cada notícia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66652b26-5171-48e8-a22c-7e5ebe692443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts.representacao_computacional import aplica_word2vec\n",
    "# modelo, embeddings = aplica_word2vec(df, nome_coluna='headline')\n",
    "\n",
    "if (usar_word2vec):\n",
    "    import subprocess\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    \n",
    "    # Salvar o dataframe temporariamente\n",
    "    df.to_parquet(\"temp_input.parquet\", engine=\"pyarrow\")  \n",
    "    \n",
    "    # Comando para ativar conda env e rodar o script\n",
    "    subprocess.run([\n",
    "        \"conda\", \"run\", \"-n\", \"word2vec_env\", \"python\",\n",
    "        \"scripts/word2vec_runner.py\", \"0\", \"temp_input.parquet\", \"text\", \"skip-gram\"\n",
    "    ])\n",
    "    \n",
    "    # Recuperar o resultado\n",
    "    with open(\"embeddings_output.pkl\", \"rb\") as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    with open(\"indices_validos.pkl\", \"rb\") as f:\n",
    "        indices_validos = pickle.load(f)\n",
    "    \n",
    "    embeddings\n",
    "    print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a6f7de",
   "metadata": {},
   "source": [
    "###### Treinar um modelo de ML tradicional usando os embeddings Word2Vec\n",
    "\n",
    "Criados os embeddings, o próximo passo é iniciar o treinamento de um modelo de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b190d36",
   "metadata": {},
   "source": [
    "Divisão de dados de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb7853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (usar_word2vec):\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Convertendo para arrays\n",
    "    X = np.array(embeddings)\n",
    "    y = df.iloc[indices_validos][\"is_sarcastic\"].astype(int).values\n",
    "    \n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    \n",
    "    # Confirma que estão alinhados\n",
    "    assert len(X) == len(y)\n",
    "    print(len(X), len(y))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0072d462",
   "metadata": {},
   "source": [
    "Testa diversos modelos/algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f0bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (usar_word2vec):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    modelos = {\n",
    "        \"SVM\": SVC(kernel='linear', probability=True),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(),\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "    }\n",
    "    \n",
    "    for nome, modelo in modelos.items():\n",
    "        print(f\"\\n=== {nome} ===\")\n",
    "        modelo.fit(X_train, y_train)\n",
    "        y_pred = modelo.predict(X_test)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05040df",
   "metadata": {},
   "source": [
    "Os resultados foram similares, porém o melhor foi o algoritmo Random Forest, sendo esse o escolhido para o modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e112c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (usar_word2vec):\n",
    "    import joblib\n",
    "\n",
    "    modelo = RandomForestClassifier(n_estimators=100)\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    joblib.dump(modelo, \"modelo_word2vec.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be96c879",
   "metadata": {},
   "source": [
    "### Predição de sarcasmo usando o modelo gerado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816922de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (usar_word2vec):\n",
    "    from scripts.preprocessamento import pre_processamento_frase\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import subprocess\n",
    "    \n",
    "    # Carrega modelo Word2Vec\n",
    "    # w2v_model = Word2Vec.load(\"modelo_word2vec.model\")\n",
    "    \n",
    "    # Carrega classificador treinado (SVM, Random Forest etc.)\n",
    "    classificador = joblib.load(\"modelo_word2vec.pkl\")\n",
    "    \n",
    "    frase = input(\"Digite um texto para análise: \")\n",
    "    \n",
    "    tokens = pre_processamento_frase(frase)\n",
    "    \n",
    "    # salvar os tokens em um arquivo CSV temporário\n",
    "    pd.DataFrame({\"tokens\": [tokens]}).to_csv(\"frase_processada.csv\", index=False)\n",
    "    \n",
    "    # Roda word2vec na frase processada\n",
    "    \n",
    "    subprocess.run([\n",
    "            \"conda\", \"run\", \"-n\", \"word2vec_env\", \"python\",\n",
    "            \"scripts/word2vec_runner.py\", \"1\"\n",
    "        ], check=True, capture_output=True)\n",
    "    \n",
    "    # Carregar os embeddings do arquivo CSV\n",
    "    vetor = pd.read_csv(\"vetor_word2vec.csv\", header=None).values\n",
    "    # print(f\"[INFO] Vetor carregado do CSV: {vetor_carregado}\")\n",
    "    \n",
    "    # vetor = vetor_medio(tokens, w2v_model)\n",
    "    \n",
    "    \n",
    "    # Previsão\n",
    "    pred = classificador.predict(vetor)\n",
    "    prob = classificador.predict_proba(vetor)[0]\n",
    "    \n",
    "    if pred[0] == 1:\n",
    "        print(f\"Sarcamo detectado (confiança: {prob[1]:.2f})\")\n",
    "    else:\n",
    "        print(f\"Sarcamo não detectado (confiança: {prob[0]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f82be05-2c38-4f05-a7b1-670f0e32a00a",
   "metadata": {},
   "source": [
    "## Segunda abordagem para detecção: Fine-tuning de um modelo Sentence Transformer\n",
    "\n",
    "A segunda abordagem é composta pela escolha de um modelo Transformrers de linguagem, e a partir dele realizer um fine-tuning pra o nosso objetivo.\n",
    "\n",
    "\"Finetuning Sentence Transformer models often heavily improves the performance of the model on your use case, because each task requires a different notion of similarity.\"\n",
    "Fonte: https://sbert.net/docs/sentence_transformer/training_overview.html\n",
    "\n",
    "Antes da aplicação do fine tuning, é importante que o dataset esteja de acordo com a função de perda.\n",
    "\"It is important that your dataset format matches your loss function (or that you choose a loss function that matches your dataset format)\"\n",
    "\n",
    "Para textos curtos (como é o exemplo da headline), o Word2Vec funciona bem. Para textos longos (como é o caso de notícias), pode ser mais efetivo utilizar transformers como BERT.\n",
    "\n",
    "Encontrar um modelo Sequence Transformer:\n",
    "- Treinado ou adaptado para pt-BR\n",
    "- Ser fine-tuning em sentence similarity, feature extraction\n",
    "- Treinado preferenciamente em notícias\n",
    "- Usar uma arquitetura encoder compatível com sentence-transformers\n",
    "\n",
    "\n",
    "Assim foi escolhido o modelo sentence-transformers/xlm-r-bert-base-nli-stsb-mean-tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a76d1",
   "metadata": {},
   "source": [
    "Carrega o modelo original e realiza o ajuste fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7758206a-26a0-46d4-820c-426828cd0cad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (usar_sequence_transformer):\n",
    "    import subprocess\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    print('  Carregando modelo base...')\n",
    "    # Carrega modelo base\n",
    "    #modelo = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    modelo = SentenceTransformer(\"sentence-transformers/xlm-r-bert-base-nli-stsb-mean-tokens\")\n",
    "    \n",
    "    # Salva o modelo para ser reutilizado no subprocesso\n",
    "    modelo.save(\"modelo_temporario_transformer\")\n",
    "    \n",
    "    # Salva o DataFrame temporariamente\n",
    "    print('Salvando o DataFrame temporário...')\n",
    "    df.to_parquet(\"temp_input.parquet\")\n",
    "    print('DataFrame temporário salvo.')\n",
    "    \n",
    "    # Executa o subprocesso\n",
    "    print('Iniciando execução do subprocesso...')\n",
    "    results = subprocess.run([\n",
    "        \"conda\", \"run\", \"-n\", \"transformers_env\", \"python\", \"-u\",\n",
    "        \"../src/scripts/fine_tuning.py\", \"temp_input.parquet\", \"modelo_temporario_transformer\"\n",
    "    ], check=True)\n",
    "    print('Execução do subprocesso finalizada.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fd6544",
   "metadata": {},
   "source": [
    "Carrega o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7dbf85-6c30-4a0f-aaf8-f843a19282d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "carregar_modelo = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d2da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if carregar_modelo:\n",
    "    import os\n",
    "    import joblib\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    # Caminhos dos arquivos salvos\n",
    "    MODELO_DIR = \"../modelos/modelo_finetunado_sarcasmo\"\n",
    "    CLASSIFICADOR_PATH = os.path.join(MODELO_DIR, \"classificador_logreg.pkl\")\n",
    "    \n",
    "    def carregar_modelo():\n",
    "        if not os.path.exists(MODELO_DIR):\n",
    "            raise FileNotFoundError(f\"Diretório '{MODELO_DIR}' não encontrado.\")\n",
    "        if not os.path.exists(CLASSIFICADOR_PATH):\n",
    "            raise FileNotFoundError(f\"Classificador '{CLASSIFICADOR_PATH}' não encontrado.\")\n",
    "    \n",
    "        print(\"[INFO] Carregando modelo e classificador...\")\n",
    "        modelo = SentenceTransformer(MODELO_DIR)\n",
    "        classificador = joblib.load(CLASSIFICADOR_PATH)\n",
    "        return modelo, classificador\n",
    "    \n",
    "    \n",
    "    modelo, classificador = carregar_modelo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e4b9ba",
   "metadata": {},
   "source": [
    "Predição de sarcasmo usando o modelo gerado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c3d1ae-9633-481f-8aeb-4127ab8a7064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy\n",
    "\n",
    "def carregar_modelo():\n",
    "    if not os.path.exists(MODELO_DIR):\n",
    "        raise FileNotFoundError(f\"Diretório '{MODELO_DIR}' não encontrado.\")\n",
    "    if not os.path.exists(CLASSIFICADOR_PATH):\n",
    "        raise FileNotFoundError(f\"Classificador '{CLASSIFICADOR_PATH}' não encontrado.\")\n",
    "\n",
    "    print(\"[INFO] Carregando modelo e classificador...\")\n",
    "    modelo = SentenceTransformer(MODELO_DIR)\n",
    "    classificador = joblib.load(CLASSIFICADOR_PATH)\n",
    "    return modelo, classificador\n",
    "\n",
    "\n",
    "def prever_sarcasmo(frase, modelo, classificador, limiar=0.5):\n",
    "    # embedding = modelo.encode([frase], convert_to_tensor=True).cpu().numpy()\n",
    "    embedding = modelo.encode([frase], convert_to_tensor=True).cpu().tolist()\n",
    "    prob = classificador.predict_proba(embedding)[0][1]  # Probabilidade de sarcasmo\n",
    "\n",
    "    if prob >= limiar:\n",
    "        return \"Sarcasmo detectado\", prob\n",
    "    else:\n",
    "        return \"Sarcasmo não detectado\", prob\n",
    "\n",
    "\n",
    "# print(\"\\nDigite uma frase para detectar sarcasmo:\")\n",
    "\n",
    "# frase = input(\"\\n> \")\n",
    "\n",
    "# if len(frase.strip()) == 0:\n",
    "#     print(\"[ERRO] Frase vazia. Tente novamente.\")\n",
    "\n",
    "# resultado, prob = prever_sarcasmo(frase, modelo, classificador)\n",
    "# print(f\"{resultado} (confiança: {prob:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d59897-d6a4-4fe1-8c07-821a51eda347",
   "metadata": {},
   "source": [
    "# Parte 2: Detecção de Ambiguidade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5955e9c-f458-4867-813f-d081b8253e07",
   "metadata": {},
   "source": [
    "# Reescrita de frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccbf0f7-1ff8-4b90-ba45-6389654e4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def verificarAmbiguidadePalavra(palavra, contexto):\n",
    "    result = subprocess.run(\n",
    "        [\n",
    "            \"conda\", \"run\", \"-n\", \"ambiguidade\", \"python\",\n",
    "            \"../src/ambiguidade.py\", contexto, palavra\n",
    "        ],\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True  # Para já retornar string ao invés de bytes\n",
    "    )\n",
    "\n",
    "    if (result.stdout.strip() == 'None'):\n",
    "        return [False, \"\"]\n",
    "\n",
    "    return [True, result.stdout.strip()]\n",
    "\n",
    "def verificarIroniaFrase(frase):\n",
    "    if len(frase.strip()) == 0:\n",
    "        print(\"[ERRO] Frase vazia. Tente novamente.\")\n",
    "\n",
    "    print('Frase: ', frase)\n",
    "    resultado, prob = prever_sarcasmo(frase, modelo, classificador)\n",
    "    print('Resultado: ', resultado)\n",
    "\n",
    "    if resultado.strip() == 'Sarcasmo detectado':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec2e281-1e6a-4048-aa20-b611568a883a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "API_KEY = ''\n",
    "genai.configure(api_key = API_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "from src.reescrita import frases\n",
    "from src.reescrita import palavras\n",
    "from src.reescrita import gerarPrompt\n",
    "#from scripts.reescrita import gerar_texto_com_lmstudio\n",
    "\n",
    "from src.avaliacao import Avaliacao\n",
    "\n",
    "# --- Loop Principal do Programa ---\n",
    "\n",
    "avaliacao = Avaliacao()\n",
    "\n",
    "while(1):\n",
    "    print(\"\\n------------------------------------------------------\\n\")\n",
    "    print(\"Digite um texto para análise e reescrita (-1 para finalizar):\")\n",
    "    texto_original = input()\n",
    "\n",
    "    if texto_original == \"-1\":\n",
    "       break\n",
    "\n",
    "    # 1. Identificação de elementos problemáticos (sarcasmo e ambiguidade)\n",
    "    palavras_ambiguas_por_frase = {}\n",
    "    frases_ironicas = []\n",
    "\n",
    "    lista_frases = frases(texto_original)\n",
    "    \n",
    "    for frase in lista_frases:\n",
    "        if verificarIroniaFrase(frase) == True:\n",
    "            frases_ironicas.append(frase)\n",
    "        \n",
    "        listapalavrasfrase = palavras(frase)\n",
    "        palavrasAmbiguasNaFrase = []\n",
    "        for palavrafrase in listapalavrasfrase:\n",
    "            resultadoambiguidade = verificarAmbiguidadePalavra(palavrafrase, frase)\n",
    "            if resultadoambiguidade[0] == True:\n",
    "                palavrasAmbiguasNaFrase.append((palavrafrase, resultadoambiguidade[1]))\n",
    "        \n",
    "        if palavrasAmbiguasNaFrase:\n",
    "            palavras_ambiguas_por_frase[frase] = palavrasAmbiguasNaFrase\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"\\n--- Itens Detectados para o Prompt ---\")\n",
    "    print(f\"Frases Irônicas: {frases_ironicas}\")\n",
    "    print(f\"Palavras Ambíguas por Frase: {palavras_ambiguas_por_frase}\")\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "    # 2. Geração do Prompt Otimizado\n",
    "    prompt_final = gerarPrompt(texto_original, frases_ironicas, palavras_ambiguas_por_frase)\n",
    "    #print(prompt_final)\n",
    "\n",
    "    # 3. Geração do Texto Tratado pelo LLM (GEMINI)\n",
    "    print(\"\\n--- Gerando texto com Gemini (Modelo hard-coded: gemini-2.5-flash) ---\")\n",
    "    texto_reescrito = model.generate_content(prompt_final).text\n",
    "    #texto_reescrito = gerar_texto_com_lmstudio(prompt_final)\n",
    "\n",
    "    if texto_reescrito:\n",
    "        if texto_reescrito.strip().startswith(\"TEXTO REESCRITO:\"):\n",
    "            texto_reescrito = texto_reescrito.strip()[len(\"TEXTO REESCRITO:\"):].strip()\n",
    "        \n",
    "        if texto_reescrito.startswith('\"') and texto_reescrito.endswith('\"'):\n",
    "            texto_reescrito = texto_reescrito[1:-1].strip()\n",
    "\n",
    "        print(\"\\n--- TEXTO REESCRITO ---\")\n",
    "        print(texto_reescrito)\n",
    "        print(\"-----------------------\")\n",
    "\n",
    "        print(avaliacao.avaliarReescrita(texto_original, texto_reescrito))\n",
    "    else:\n",
    "        print(\"\\nNão foi possível gerar o texto reescrito.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Main)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

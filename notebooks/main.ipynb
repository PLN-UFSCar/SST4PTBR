{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f516a953-3599-45de-9ed4-3286ef8b4383",
   "metadata": {},
   "source": [
    "# Sarcasm Detection\n",
    "\n",
    "The objective of this notebook is to develop an automatic sarcasm detection module for texts in Portuguese, specifically news articles. For this purpose, a dataset composed of news articles from three major Brazilian websites was used, containing both sarcastic and non-sarcastic news. To create the module, two classifier models were developed using different methodologies: the use of classical Machine Learning algorithms + static vector representation, and fine-tuning of a multilingual transformers model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0543dc3d-2d0f-4683-a310-eb4026fd5661",
   "metadata": {},
   "source": [
    "## Description of the data set structure and characteristics\n",
    "\n",
    "The database was taken from the [PLNCrawler repository] (https://github.com/schuberty/PLNCrawler), and is originally structured in three JSON files, which correspond to each news site from where the news were extracted:\n",
    "- Sensationalista: 5006 sarcastic news\n",
    "- Estadão: 11272 non-sarcastic news\n",
    "- Revista Piauí (Herald section): 2216 sarcastic news\n",
    "\n",
    "Each file has the following fields for each news:\n",
    "- is_sarcastic (or is_sarcasm): boolean, represents the label/label of the news (sarcastic or not)\n",
    "- article_link: string, contains the URL where the news was extracted\n",
    "- headline: string, contains the news title\n",
    "-text: string, contains the news text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73077a2e",
   "metadata": {},
   "source": [
    "Loads the datasets of each site in DataFrame format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d55fb6-2d0c-4579-b677-7b413a736dde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from src.scripts import get_df_sensacionalista\n",
    "from src.scripts import get_df_estadao\n",
    "from src.scripts import get_df_the_piaui_herald\n",
    "\n",
    "# Load the file in a DataFrame\n",
    "df_sensacionalista = get_df_sensacionalista()\n",
    "df_estadao = get_df_estadao()\n",
    "df_piaui = get_df_the_piaui_herald()\n",
    "df_piaui = df_piaui.rename(columns={'is_sarcasm': 'is_sarcastic'}) # Rename the column to equalize with other DataFrames\n",
    "\n",
    "display(df_sensacionalista)\n",
    "display(df_estadao)\n",
    "display(df_piaui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0763876-e34b-4bf6-bdeb-65d1eaa8178e",
   "metadata": {},
   "source": [
    "Makes the union of the three bases in a single DataFrame in a balanced way, keeping 50% of sarcastic news and 50% of non-sarcastic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960f4cd-3e68-4cfe-a454-53523fe11614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the 3 datasets\n",
    "from src.scripts import merge_dfs\n",
    "\n",
    "df = merge_dfs(df_sensacionalista, df_estadao, df_piaui)\n",
    "\n",
    "num_sarcastic = df['is_sarcastic'].sum()\n",
    "\n",
    "print(f'Number of sarcastic samples: {num_sarcastic}')\n",
    "print(f'Number of non-sarcastic samples: {len(df) - num_sarcastic}')\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5090085-21ee-4fde-a10a-c1176905a9b1",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "It is important to point out that some language resources that are removed or normalized during the traditional pre-processing steps influence the classification of irony in texts.\n",
    "For example, punctuation marks may indicate irony. Therefore, it is a pre-processing parameter to remove or not this feature.\n",
    "\n",
    "Knowing this, optional parameters can be passed to the pre-processing function that apply or not the transformation.\n",
    "\n",
    "## Stemming and lemmatization\n",
    "\n",
    "> \"Stemming or lemmatization reduces words to their root form (e.g., \"running\" becomes \"run\"), making it easier to analyze language by grouping different forms of the same word.\" Source: https://www.ibm.com/think/topics/natural-language-processing\n",
    "\n",
    "The process of stemming and lemmatization are optional, but both can never be applied together because they have the same purpose with different approaches.\n",
    "Thus, if both are activated only **lemmatization** will be applied (because it is more semantic).\n",
    "\n",
    "### Sources for pre-processing:\n",
    "\n",
    "1. [Key Guidelines](https://github.com/sharadpatell/Text_preprocessing_steps_for_NLP/blob/main/Text_preprocessing_steps_for_NLP.ipynb) which assisted in the step-by-step pre-processing.\n",
    "2. FACELI, K. et al. Artificial Intelligence An Approach to Machine Learning. 2nd edition ed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5387a47-0606-48fa-ae53-64d9faf76ec0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.preprocessamento import pre_processamento\n",
    "\n",
    "use_lemmatization = True\n",
    "use_stemming      = False\n",
    "\n",
    "df = pre_processamento(df, usar_stemming = use_stemming, usar_lemmatization = use_lemmatization)\n",
    "\n",
    "# Saves the DataFrame temporarily\n",
    "print('Saving the temporary DataFrame...')\n",
    "df.to_parquet(\"../temp/temp_input.parquet\")\n",
    "print('Temporary DataFrame saved.')\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7899157-7fc8-4692-891d-451fee3b3ebf",
   "metadata": {},
   "source": [
    "## First approach to detection: Use of classic machine learning algorithms + static vector representation\n",
    "\n",
    "For this approach, the first step is to create a vector representation of text, because computers do not interpret texts in human language. Therefore, it is necessary to transform them into a structured representation that the machines can process.\n",
    "\n",
    "This text treatment is also part of the feature extraction step.\n",
    "> Feature extraction is the process of converting raw text into numerical representations that machines can analyze and interpret. Source: https://www.ibm.com/think/topics/natural-language-processing\n",
    "\n",
    "It was chosen to use the Word2Vec tool for the generation of dense vectors, which capture the semantic value of words and relate them to each other. It is ideal for machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5232b5-8124-4260-939c-c2d77e859fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_word2vec = False\n",
    "use_sequence_transformer = not use_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da54478",
   "metadata": {},
   "source": [
    "Applies Word2Vec in the database, generating the neural network and returning the embeddings for each news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66652b26-5171-48e8-a22c-7e5ebe692443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if (use_word2vec):\n",
    "    import subprocess\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    \n",
    "    # Runs Word2Vec in a separate conda environment\n",
    "    subprocess.run([\n",
    "        \"conda\", \"run\", \"-n\", \"word2vec_env\", \"python\",\n",
    "        \"../src/word2vec_runner.py\", \"0\", \"../temp/temp_input.parquet\", \"text\", \"skip-gram\"\n",
    "    ])\n",
    "    \n",
    "    # Retrieve the resulting embeddings\n",
    "    with open(\"../temp/embeddings_output.pkl\", \"rb\") as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    with open(\"../temp/indices_validos.pkl\", \"rb\") as f:\n",
    "        valid_indexes = pickle.load(f)\n",
    "    \n",
    "    embeddings\n",
    "    print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a6f7de",
   "metadata": {},
   "source": [
    "#### Train a traditional ML model using Word2Vec embeddings\n",
    "\n",
    "Created the embeddings, the next step is to start training a Machine Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b190d36",
   "metadata": {},
   "source": [
    "Split of training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb7853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (use_word2vec):\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Converting to arrays\n",
    "    X = np.array(embeddings)\n",
    "    y = df.iloc[valid_indexes][\"is_sarcastic\"].astype(int).values\n",
    "    \n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    \n",
    "    # Confirms that they are aligned\n",
    "    assert len(X) == len(y)\n",
    "    print(len(X), len(y))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0072d462",
   "metadata": {},
   "source": [
    "Tests several models/algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f0bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (use_word2vec):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    models = {\n",
    "        \"SVM\": SVC(kernel='linear', probability=True),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(),\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "    }\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05040df",
   "metadata": {},
   "source": [
    "The results were similar, but the best was the Random Forest algorithm, which was chosen for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e112c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (use_word2vec):\n",
    "    import joblib\n",
    "\n",
    "    modelo = RandomForestClassifier(n_estimators=100)\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    joblib.dump(modelo, \"../modelos/classificador_word2vec.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be96c879",
   "metadata": {},
   "source": [
    "### Prediction of sarcasm using the generated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816922de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (use_word2vec):\n",
    "    from src.preprocessamento import pre_processamento_frase\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import subprocess\n",
    "\n",
    "    \n",
    "    # Loads trained classifier (SVM, Random Forest etc.)\n",
    "    classificador = joblib.load(\"../modelos/classificador_word2vec.pkl\")\n",
    "    \n",
    "    frase = input(\"Enter a text for analysis: \")\n",
    "    \n",
    "    tokens = pre_processamento_frase(frase)\n",
    "    \n",
    "    # Saves the tokens in a temporary CSV file\n",
    "    pd.DataFrame({\"tokens\": [tokens]}).to_csv(\"../temp/frase_processada.csv\", index=False)\n",
    "    \n",
    "    # Run Word2Vec in the processed phrase\n",
    "    subprocess.run([\n",
    "            \"conda\", \"run\", \"-n\", \"word2vec_env\", \"python\",\n",
    "            \"../src/word2vec_runner.py\", \"1\"\n",
    "        ], check=True, capture_output=True)\n",
    "    \n",
    "    # Load the CSV file embeddings\n",
    "    vetor = pd.read_csv(\"../temp/vetor_word2vec.csv\", header=None).values\n",
    "    \n",
    "\n",
    "    # Prediction\n",
    "    pred = classificador.predict(vetor)\n",
    "    prob = classificador.predict_proba(vetor)[0]\n",
    "    \n",
    "    if pred[0] == 1:\n",
    "        print(f\"Sarcasm detected (trust: {prob[1]:.2f})\")\n",
    "    else:\n",
    "        print(f\"Sarcasm not detected (trust: {prob[0]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f82be05-2c38-4f05-a7b1-670f0e32a00a",
   "metadata": {},
   "source": [
    "## Second approach to detection: Fine-tuning a Sentence Transformer model\n",
    "\n",
    "The second approach consists of choosing a Transformrers language model, and from it perform a fine-tuning to our goal.\n",
    "\n",
    "\"Finetuning Sentence Transformer models often heavily improves the performance of the model on your use case, because each task requires a different notion of similarity.\"\n",
    "Source: https://sbert.net/docs/sentence_transformer/training_overview.html\n",
    "\n",
    "Before applying fine tuning, it is important that the dataset be in accordance with the loss function.\n",
    "\"It is important that your dataset format matches your loss function (or that you choose a loss function that matches your dataset format)\"\n",
    "\n",
    "For short texts (like the headline example), Word2Vec works well. For long texts (such as news), it may be more effective to use transformers like BERT.\n",
    "\n",
    "Find a Sequence Transformer model:\n",
    "- Trained or adapted for pt-BR\n",
    "- Fine-tuning in sentence similarity, feature extraction\n",
    "- Trained preferably in news\n",
    "- Use an encoder architecture compatible with sentence-transformers\n",
    "\n",
    "\n",
    "Thus the model sentence-transformers/xlm-r-bert-base-nli-stsb-mean-tokens was chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a76d1",
   "metadata": {},
   "source": [
    "To fine-tune, upload and run the notebook [_fine_tuning.ipynb_](fine_tuning.ipynb) in the Google Colab environment. Follow all instructions on the notebook.\n",
    "\n",
    "After execution, save the trained model in the 'models' folder of this repository. Don’t forget to unzip the file .zip, and then continue running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fd6544",
   "metadata": {},
   "source": [
    "Loads the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d2da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "if use_sequence_transformer:\n",
    "\n",
    "    # Paths of the saved files\n",
    "    MODEL_DIR = \"../modelos/modelo_finetunado_sarcasmo\"\n",
    "    CLASSIFIER_PATH = os.path.join(MODEL_DIR, \"/classificador_logreg.pkl\")\n",
    "\n",
    "    def load_model():\n",
    "        if not os.path.exists(MODEL_DIR):\n",
    "            raise FileNotFoundError(f\"Directory '{MODEL_DIR}' not found.\")\n",
    "        if not os.path.exists(CLASSIFIER_PATH):\n",
    "            raise FileNotFoundError(f\"Classifier '{CLASSIFIER_PATH}' not found.\")\n",
    "\n",
    "        print(\"Loading model and classifier...\")\n",
    "        model = SentenceTransformer(MODEL_DIR)\n",
    "        classifier = joblib.load(CLASSIFIER_PATH)\n",
    "        return model, classifier\n",
    "\n",
    "\n",
    "    model, classifier = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e4b9ba",
   "metadata": {},
   "source": [
    "Prediction of sarcasm using the generated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c3d1ae-9633-481f-8aeb-4127ab8a7064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy\n",
    "\n",
    "if (use_sequence_transformer):\n",
    "\n",
    "    def predict_sarcasm(text, model, classifier, threshold=0.5):\n",
    "        embedding = model.encode([text], convert_to_tensor=True).cpu().tolist()\n",
    "        prob = classifier.predict_proba(embedding)[0][1]  # Probability of sarcasm\n",
    "\n",
    "        if prob >= threshold:\n",
    "            return \"Sarcasm detected\", prob\n",
    "        else:\n",
    "            return \"Sarcasm not detected\", prob\n",
    "\n",
    "\n",
    "    print(\"\\nType a text to detect sarcasm:\")\n",
    "\n",
    "    text = input(\"\\n> \")\n",
    "\n",
    "    if len(text.strip()) == 0:\n",
    "        print(\"Empty sentence. Try again.\")\n",
    "\n",
    "    result, prob = predict_sarcasm(text, model, classifier)\n",
    "    print(f\"{result} (trust: {prob:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d59897-d6a4-4fe1-8c07-821a51eda347",
   "metadata": {},
   "source": [
    "# Part 2: Ambiguity Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5955e9c-f458-4867-813f-d081b8253e07",
   "metadata": {},
   "source": [
    "## Rewrite of phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccbf0f7-1ff8-4b90-ba45-6389654e4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def checkAmbiguityWord(word, context):\n",
    "    result = subprocess.run(\n",
    "        [\n",
    "            \"conda\", \"run\", \"-n\", \"ambiguidade\", \"python\",\n",
    "            \"../src/ambiguidade.py\", context, word\n",
    "        ],\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True  # To already return string instead of bytes\n",
    "    )\n",
    "\n",
    "    if (result.stdout.strip() == 'None'):\n",
    "        return [False, \"\"]\n",
    "\n",
    "    return [True, result.stdout.strip()]\n",
    "\n",
    "def checkSarcasmSentence(sentence):\n",
    "    if len(sentence.strip()) == 0:\n",
    "        print(\"[ERRO] Frase vazia. Tente novamente.\")\n",
    "\n",
    "    print('Sentence: ', sentence)\n",
    "    result, prob = predict_sarcasm(sentence, model, classifier)\n",
    "    print('Result: ', result)\n",
    "\n",
    "    if result.strip() == 'Sarcasm detected':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec2e281-1e6a-4048-aa20-b611568a883a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "API_KEY = ''\n",
    "genai.configure(api_key = API_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "from src.reescrita import frases\n",
    "from src.reescrita import palavras\n",
    "from src.reescrita import gerarPrompt\n",
    "#from scripts.reescrita import gerar_texto_com_lmstudio\n",
    "\n",
    "from src.avaliacao import Avaliacao\n",
    "\n",
    "# --- Main Program Loop ---\n",
    "\n",
    "avaliacao = Avaliacao()\n",
    "\n",
    "while(1):\n",
    "    print(\"\\n------------------------------------------------------\\n\")\n",
    "    print(\"Enter a text for analysis and rewrite (-1 to finish):\")\n",
    "    original_text = input()\n",
    "\n",
    "    if original_text == \"-1\":\n",
    "       break\n",
    "\n",
    "    # 1. Identification of problematic elements (sarcasm and ambiguity)\n",
    "    ambiguous_words_per_sentence = {}\n",
    "    sarcastic_sentences = []\n",
    "\n",
    "    sentences_list = frases(original_text)\n",
    "    \n",
    "    for sentence in sentences_list:\n",
    "        if checkSarcasmSentence(sentence) == True:\n",
    "            sarcastic_sentences.append(sentence)\n",
    "        \n",
    "        sentence_words_list = palavras(sentence)\n",
    "        ambiguous_words_sentence = []\n",
    "        for sentence_word in sentence_words_list:\n",
    "            ambiguity_result= checkAmbiguityWord(sentence_word, sentence)\n",
    "            if ambiguity_result[0] == True:\n",
    "                ambiguous_words_sentence.append((sentence_word, ambiguity_result[1]))\n",
    "        \n",
    "        if ambiguous_words_sentence:\n",
    "            ambiguous_words_per_sentence[frase] = ambiguous_words_sentence\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"\\n--- Detected Items for the Prompt ---\")\n",
    "    print(f\"Sarcastic sentences: {sarcastic_sentences}\")\n",
    "    print(f\"Ambiguous words per sentence : {ambiguous_words_per_sentence}\")\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "    # 2. Optimized Prompt Generation\n",
    "    final_prompt = gerarPrompt(original_text, sarcastic_sentences, ambiguous_words_per_sentence)\n",
    "\n",
    "    # 3. Generation of the Treated Text by the LLM (GEMINI)\n",
    "    print(\"\\n--- Generating text with Gemini (Hard-coded model: gemini-2.5-flash) ---\")\n",
    "    rewritten_text = model.generate_content(final_prompt).text\n",
    "\n",
    "    if rewritten_text:\n",
    "        if rewritten_text.strip().startswith(\"TEXTO REESCRITO:\"):\n",
    "            rewritten_text = rewritten_text.strip()[len(\"TEXTO REESCRITO:\"):].strip()\n",
    "        \n",
    "        if rewritten_text.startswith('\"') and rewritten_text.endswith('\"'):\n",
    "            rewritten_text = rewritten_text[1:-1].strip()\n",
    "\n",
    "        print(\"\\n--- REWRITTEN TEXT ---\")\n",
    "        print(rewritten_text)\n",
    "        print(\"-----------------------\")\n",
    "\n",
    "        print(avaliacao.avaliarReescrita(original_text, rewritten_text))\n",
    "    else:\n",
    "        print(\"\\nCould not generate the rewritten text.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Main)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
